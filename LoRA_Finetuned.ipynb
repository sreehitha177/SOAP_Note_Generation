{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q evaluate bert_score rouge_score sacremoses\n"
      ],
      "metadata": {
        "id": "5joF0RS2CwE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# DEPENDENCIES\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import logging\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "AutoTokenizer,\n",
        "AutoModelForSeq2SeqLM,\n",
        "DataCollatorForSeq2Seq,\n",
        "Seq2SeqTrainingArguments,\n",
        "Seq2SeqTrainer,\n",
        "GenerationConfig,\n",
        ")\n",
        "from peft import (\n",
        "LoraConfig,\n",
        "get_peft_model,\n",
        "TaskType,\n",
        "PeftModel,\n",
        "PeftConfig,\n",
        ")\n",
        "\n",
        "\n",
        "import evaluate\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# GLOBAL CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "MODEL_NAME = \"google/flan-t5-large\"\n",
        "DATASET_ID = \"omi-health/medical-dialogue-to-soap-summary\"\n",
        "OUTPUT_DIR = \"./flan_t5_large_soap_lora\"\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/KG_Medical_SOAP_Model\"\n",
        "\n",
        "\n",
        "MAX_INPUT_LENGTH = 1024\n",
        "MAX_TARGET_LENGTH = 512\n",
        "MAX_EVAL_SAMPLES = 100\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SETUP\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"punkt_tab\", quiet=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "set_seed(SEED)"
      ],
      "metadata": {
        "id": "-vyD75oExDml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# DATA LOADING\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "raw_ds = load_dataset(DATASET_ID)\n",
        "\n",
        "\n",
        "if \"validation\" not in raw_ds:\n",
        "    split = raw_ds[\"train\"].train_test_split(test_size=0.1, seed=SEED)\n",
        "    raw_ds[\"train\"] = split[\"train\"]\n",
        "    raw_ds[\"validation\"] = split[\"test\"]"
      ],
      "metadata": {
        "id": "X62EhKROxDps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# TOKENIZER & MODEL\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    MODEL_NAME, torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q\", \"v\"],\n",
        ")\n",
        "\n",
        "\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "N5AIrnazxDs1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PREPROCESSING\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def preprocess_function(batch):\n",
        "    prompts = [\n",
        "        \"Generate a structured medical SOAP note from this doctor-patient dialogue:\\n\\n\" + d\n",
        "        for d in batch[\"dialogue\"]\n",
        "    ]\n",
        "\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        prompts, truncation=True, max_length=MAX_INPUT_LENGTH\n",
        "    )\n",
        "\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            batch[\"soap\"], truncation=True, max_length=MAX_TARGET_LENGTH\n",
        "        )\n",
        "\n",
        "\n",
        "    labels[\"input_ids\"] = [\n",
        "        [(tok if tok != tokenizer.pad_token_id else -100) for tok in seq]\n",
        "        for seq in labels[\"input_ids\"]\n",
        "    ]\n",
        "\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tokenized_ds = raw_ds.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=raw_ds[\"train\"].column_names,\n",
        ")"
      ],
      "metadata": {
        "id": "HQryJMTPAr0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# MEDICAL ENTITY EXTRACTION (HALLUCINATION)\n",
        "# =============================================================================\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "ner_pipeline = pipeline(\n",
        "    \"ner\",\n",
        "    model=\"d4data/biomedical-ner-all\",\n",
        "    tokenizer=\"d4data/biomedical-ner-all\",\n",
        "    aggregation_strategy=\"simple\",\n",
        ")\n",
        "\n",
        "def extract_medical_terms(text: str) -> set:\n",
        "    ents = ner_pipeline(text[:2000])\n",
        "    return {e[\"word\"].lower() for e in ents}\n"
      ],
      "metadata": {
        "id": "FwsjRMEswsdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# METRICS\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(p)) for p in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(l)) for l in decoded_labels]\n",
        "\n",
        "\n",
        "    scores = rouge_metric.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=decoded_labels,\n",
        "        use_stemmer=True,\n",
        "    )\n",
        "\n",
        "\n",
        "    return {k: round(v * 100, 4) for k, v in scores.items()}\n",
        "\n",
        "\n",
        "def hallucination_score(dialogue: str, prediction: str) -> float:\n",
        "    \"\"\"\n",
        "    Fraction of medical entities in prediction\n",
        "    that do NOT appear in the input dialogue.\n",
        "    \"\"\"\n",
        "    pred_ents = extract_entities(prediction)\n",
        "    input_ents = extract_entities(dialogue)\n",
        "\n",
        "    if not pred_ents:\n",
        "        return 0.0\n",
        "\n",
        "    hallucinated = pred_ents - input_ents\n",
        "    return len(hallucinated) / len(pred_ents)"
      ],
      "metadata": {
        "id": "M1I5iFhRAr38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=1e-3,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    bf16=True,\n",
        "    predict_with_generate=True,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"rougeL\",\n",
        "    generation_config=GenerationConfig.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        max_new_tokens=512,\n",
        "        repetition_penalty=2.5,\n",
        "        no_repeat_ngram_size=3,\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_ds[\"train\"],\n",
        "    eval_dataset=tokenized_ds[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "rOwzlK4ZAr7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# GENERATION\n",
        "# =============================================================================\n",
        "\n",
        "def generate_soap(model, tokenizer, dialogue: str, device: str) -> str:\n",
        "    prompt = (\n",
        "        \"Generate a structured medical SOAP note from this doctor-patient dialogue:\\n\\n\"\n",
        "        + dialogue\n",
        "    )\n",
        "\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_INPUT_LENGTH,\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=MAX_TARGET_LENGTH,\n",
        "            num_beams=3,\n",
        "            repetition_penalty=2.5,\n",
        "            no_repeat_ngram_size=3,\n",
        "        )\n",
        "\n",
        "\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "gFJssiK1AsBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# EVALUATION CLASS\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "class SOAPEvaluator:\n",
        "    def __init__(self):\n",
        "        self.rouge = evaluate.load(\"rouge\")\n",
        "        self.bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def clean(text: str) -> str:\n",
        "        text = text.lower().strip()\n",
        "        return re.sub(r\"\\s+\", \" \", text)\n",
        "\n",
        "\n",
        "    def lexical(self, refs: List[str], preds: List[str]):\n",
        "        refs = [self.clean(r) for r in refs]\n",
        "        preds = [self.clean(p) for p in preds]\n",
        "\n",
        "\n",
        "        rouge = self.rouge.compute(predictions=preds, references=refs)\n",
        "        bleu = self.bleu.compute(predictions=preds, references=[[r] for r in refs])\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"rouge1\": round(rouge[\"rouge1\"], 4),\n",
        "            \"rouge2\": round(rouge[\"rouge2\"], 4),\n",
        "            \"rougeL\": round(rouge[\"rougeL\"], 4),\n",
        "            \"bleu\": round(bleu[\"bleu\"], 4),\n",
        "        }\n",
        "\n",
        "\n",
        "    def bert(self, refs: List[str], preds: List[str]):\n",
        "        P, R, F = bert_score(preds, refs, lang=\"en\", verbose=False)\n",
        "        return {\n",
        "            \"bertscore_precision\": round(P.mean().item(), 4),\n",
        "            \"bertscore_recall\": round(R.mean().item(), 4),\n",
        "            \"bertscore_f1\": round(F.mean().item(), 4),\n",
        "        }\n",
        "\n",
        "\n",
        "    def hallucination(self, dialogues: List[str], preds: List[str]):\n",
        "        scores = [\n",
        "            hallucination_score(d, p)\n",
        "            for d, p in zip(dialogues, preds)\n",
        "        ]\n",
        "        return {\n",
        "            \"hallucination_rate\": round(float(np.mean(scores)), 4)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "zkcIlPJgxDwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FLUENCY_PROMPT = \"\"\"\n",
        "You are an experienced physician and medical editor.\n",
        "\n",
        "Evaluate the FLUENCY of the following SOAP note.\n",
        "\n",
        "Consider:\n",
        "- grammatical correctness\n",
        "- clarity and coherence\n",
        "- professional medical tone\n",
        "- readability for clinical documentation\n",
        "\n",
        "Score on a scale from 0 to 10:\n",
        "- 0 = incoherent, ungrammatical, unusable\n",
        "- 10 = perfectly fluent, natural, indistinguishable from a human physician\n",
        "\n",
        "SOAP NOTE:\n",
        "<<<\n",
        "{generated_soap}\n",
        ">>>\n",
        "\n",
        "Return ONLY a single integer score between 0 and 10.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "CONSISTENCY_PROMPT = \"\"\"\n",
        "You are an experienced physician.\n",
        "\n",
        "Evaluate the CONSISTENCY of the generated SOAP note\n",
        "with respect to the reference physician SOAP note.\n",
        "\n",
        "Consider:\n",
        "- whether key clinical facts match\n",
        "- whether important information is missing\n",
        "- whether incorrect or fabricated details are introduced\n",
        "- whether assessment and plan align with the reference\n",
        "\n",
        "Score on a scale from 0 to 10:\n",
        "- 0 = completely inconsistent or incorrect\n",
        "- 10 = fully consistent and clinically aligned\n",
        "\n",
        "REFERENCE SOAP:\n",
        "<<<\n",
        "{reference_soap}\n",
        ">>>\n",
        "\n",
        "GENERATED SOAP:\n",
        "<<<\n",
        "{generated_soap}\n",
        ">>>\n",
        "\n",
        "Return ONLY a single integer score between 0 and 10.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "cr3XMNcF0yuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-generativeai\n",
        "\n",
        "import os\n",
        "import re\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyDmKkVhVFIuBcaxPdBpiZgOBqgmmVPbE_oabcderghsn\""
      ],
      "metadata": {
        "id": "rb_3Udq3Uz-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# GEMINI JUDGING\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "\n",
        "GEMINI_MODEL = \"gemini-2.0-flash-lite\"\n",
        "judge_model = genai.GenerativeModel(GEMINI_MODEL)\n",
        "\n",
        "def parse_score(text: str) -> int:\n",
        "    match = re.search(r\"\\b([0-9]|10)\\b\", text)\n",
        "    if match:\n",
        "        return int(match.group(1))\n",
        "    return 0  # fallback\n",
        "\n",
        "\n",
        "def judge_fluency(generated_soap: str, model: str = GEMINI_MODEL) -> int:\n",
        "    prompt = FLUENCY_PROMPT.format(generated_soap=generated_soap)\n",
        "\n",
        "    response = judge_model.generate_content(\n",
        "        prompt,\n",
        "        generation_config={\n",
        "            \"temperature\": 0.0,\n",
        "            \"max_output_tokens\": 5,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    output = response.text.strip()\n",
        "    return parse_score(output)\n",
        "\n",
        "\n",
        "def judge_consistency(\n",
        "    reference_soap: str,\n",
        "    generated_soap: str,\n",
        "    model: str = GEMINI_MODEL,\n",
        ") -> int:\n",
        "    prompt = CONSISTENCY_PROMPT.format(\n",
        "        reference_soap=reference_soap,\n",
        "        generated_soap=generated_soap,\n",
        "    )\n",
        "\n",
        "    response = judge_model.generate_content(\n",
        "        prompt,\n",
        "        generation_config={\n",
        "            \"temperature\": 0.0,\n",
        "            \"max_output_tokens\": 5,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    output = response.text.strip()\n",
        "    return parse_score(output)\n"
      ],
      "metadata": {
        "id": "R_rOwCFg07DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fluency_scores = []\n",
        "consistency_scores = []\n",
        "MAX_JUDGE_SAMPLES = 50\n",
        "\n",
        "for i, (ref, pred) in enumerate(\n",
        "    list(zip(refs, preds))[:MAX_JUDGE_SAMPLES]\n",
        "):\n",
        "    try:\n",
        "        fluency = judge_fluency(pred)\n",
        "        consistency = judge_consistency(ref, pred)\n",
        "\n",
        "        fluency_scores.append(fluency)\n",
        "        consistency_scores.append(consistency)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[Judge failed at sample {i}]: {e}\")\n",
        "        break\n",
        "\n",
        "\n",
        "results = {\n",
        "    \"fluency_mean\": round(float(np.mean(fluency_scores)), 3),\n",
        "    \"fluency_std\": round(float(np.std(fluency_scores)), 3),\n",
        "    \"consistency_mean\": round(float(np.mean(consistency_scores)), 3),\n",
        "    \"consistency_std\": round(float(np.std(consistency_scores)), 3),\n",
        "}\n",
        "\n",
        "print(results)"
      ],
      "metadata": {
        "id": "nH2Mnd0DmThc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "    logger.info(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "\n",
        "    logger.info(\"Saving model...\")\n",
        "    model.save_pretrained(DRIVE_PATH)\n",
        "    tokenizer.save_pretrained(DRIVE_PATH)\n",
        "\n",
        "\n",
        "    logger.info(\"Running evaluation...\")\n",
        "    eval_ds = load_dataset(DATASET_ID, split=\"test\").select(range(MAX_EVAL_SAMPLES))\n",
        "\n",
        "\n",
        "    preds, refs = [], []\n",
        "    for sample in tqdm(eval_ds):\n",
        "        preds.append(generate_soap(model, tokenizer, sample[\"dialogue\"], device))\n",
        "        refs.append(sample[\"soap\"])\n",
        "\n",
        "\n",
        "    evaluator = SOAPEvaluator()\n",
        "    print(evaluator.lexical(refs, preds))\n",
        "    print(evaluator.bert(refs, preds))\n",
        "    print(evaluator.hallucination([sample[\"dialogue\"] for sample in eval_ds], preds))"
      ],
      "metadata": {
        "id": "KwY6vdPHBFV5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
