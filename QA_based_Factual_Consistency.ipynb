{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3QrKblKiJuX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from transformers import RobertaForQuestionAnswering, RobertaTokenizer\n",
        "from typing import List, Tuple, Dict\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "class FactualConsistencyEvaluator:\n",
        "    \"\"\"\n",
        "    QA-based factual consistency evaluation pipeline.\n",
        "    Uses a fine-tuned BART-large for question generation and\n",
        "    RoBERTa-base for question answering to verify if information\n",
        "    in generated notes is grounded in the original clinical dialogue.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 qg_model_name: str = \"facebook/bart-large\",\n",
        "                 qa_model_name: str = \"deepset/roberta-base-squad2\",\n",
        "                 device: str = None):\n",
        "        \"\"\"\n",
        "        Initialize the factual consistency evaluator.\n",
        "\n",
        "        Args:\n",
        "            qg_model_name: Name/path of the BART model for question generation\n",
        "            qa_model_name: Name/path of the RoBERTa model for QA\n",
        "            device: Device to run models on ('cuda', 'cpu', or None for auto)\n",
        "        \"\"\"\n",
        "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Initialize Question Generation model (BART-large fine-tuned)\n",
        "        print(f\"Loading QG model: {qg_model_name}\")\n",
        "        self.qg_tokenizer = BartTokenizer.from_pretrained(qg_model_name)\n",
        "        self.qg_model = BartForConditionalGeneration.from_pretrained(qg_model_name)\n",
        "        self.qg_model.to(self.device)\n",
        "        self.qg_model.eval()\n",
        "\n",
        "        # Initialize Question Answering model (RoBERTa-base)\n",
        "        print(f\"Loading QA model: {qa_model_name}\")\n",
        "        self.qa_tokenizer = RobertaTokenizer.from_pretrained(qa_model_name)\n",
        "        self.qa_model = RobertaForQuestionAnswering.from_pretrained(qa_model_name)\n",
        "        self.qa_model.to(self.device)\n",
        "        self.qa_model.eval()\n",
        "\n",
        "    def generate_questions(self, generated_note: str, num_questions: int = 5) -> List[str]:\n",
        "        \"\"\"\n",
        "        Generate factual questions from the generated SOAP note.\n",
        "\n",
        "        Args:\n",
        "            generated_note: The generated clinical note\n",
        "            num_questions: Number of questions to generate\n",
        "\n",
        "        Returns:\n",
        "            List of generated questions\n",
        "        \"\"\"\n",
        "        # Prepare prompt for question generation\n",
        "        prompt = f\"Generate {num_questions} factual questions based on this clinical note: {generated_note}\"\n",
        "\n",
        "        # Tokenize and generate questions\n",
        "        inputs = self.qg_tokenizer(prompt,\n",
        "                                  max_length=512,\n",
        "                                  truncation=True,\n",
        "                                  return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        # Generate questions\n",
        "        with torch.no_grad():\n",
        "            outputs = self.qg_model.generate(\n",
        "                **inputs,\n",
        "                max_length=200,\n",
        "                num_beams=4,\n",
        "                num_return_sequences=num_questions,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "        # Decode generated questions\n",
        "        questions = []\n",
        "        for output in outputs:\n",
        "            question = self.qg_tokenizer.decode(output, skip_special_tokens=True)\n",
        "            # Extract just the question part (remove prompt if present)\n",
        "            if \"question:\" in question.lower():\n",
        "                question = question.split(\"question:\")[-1].strip()\n",
        "            questions.append(question.strip())\n",
        "\n",
        "        return list(set(questions))[:num_questions]  # Remove duplicates\n",
        "\n",
        "    def extract_answer(self,\n",
        "                      context: str,\n",
        "                      question: str,\n",
        "                      model_type: str = 'qa') -> Tuple[str, float]:\n",
        "        \"\"\"\n",
        "        Extract answer from context using QA model.\n",
        "\n",
        "        Args:\n",
        "            context: Text to search for answer (dialogue or generated note)\n",
        "            question: Question to answer\n",
        "            model_type: Which model to use ('qa' for QA model)\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (answer_text, confidence_score)\n",
        "        \"\"\"\n",
        "        if model_type == 'qa':\n",
        "            tokenizer = self.qa_tokenizer\n",
        "            model = self.qa_model\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "        # Tokenize inputs\n",
        "        inputs = tokenizer(question,\n",
        "                          context,\n",
        "                          max_length=512,\n",
        "                          truncation=True,\n",
        "                          return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        # Get answer\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        # Get start and end positions\n",
        "        answer_start = torch.argmax(outputs.start_logits)\n",
        "        answer_end = torch.argmax(outputs.end_logits) + 1\n",
        "\n",
        "        # Calculate confidence (average of start and end logits)\n",
        "        confidence = (outputs.start_logits[0][answer_start].item() +\n",
        "                     outputs.end_logits[0][answer_end-1].item()) / 2\n",
        "\n",
        "        # Convert token indices to text\n",
        "        answer_tokens = inputs['input_ids'][0][answer_start:answer_end]\n",
        "        answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
        "\n",
        "        return answer.strip(), confidence\n",
        "\n",
        "    def calculate_f1(self, pred_answer: str, true_answer: str) -> float:\n",
        "        \"\"\"\n",
        "        Calculate F1 score between predicted and true answers.\n",
        "\n",
        "        Args:\n",
        "            pred_answer: Predicted answer\n",
        "            true_answer: True answer\n",
        "\n",
        "        Returns:\n",
        "            F1 score (0-1)\n",
        "        \"\"\"\n",
        "        # Tokenize into words (simple whitespace split)\n",
        "        pred_tokens = set(pred_answer.lower().split())\n",
        "        true_tokens = set(true_answer.lower().split())\n",
        "\n",
        "        if not pred_tokens or not true_tokens:\n",
        "            return 0.0\n",
        "\n",
        "        # Calculate precision, recall, F1\n",
        "        common_tokens = pred_tokens.intersection(true_tokens)\n",
        "        precision = len(common_tokens) / len(pred_tokens) if pred_tokens else 0\n",
        "        recall = len(common_tokens) / len(true_tokens) if true_tokens else 0\n",
        "\n",
        "        if precision + recall == 0:\n",
        "            return 0.0\n",
        "\n",
        "        f1 = 2 * (precision * recall) / (precision + recall)\n",
        "        return f1\n",
        "\n",
        "    def evaluate_consistency(self,\n",
        "                           clinical_dialogue: str,\n",
        "                           generated_note: str,\n",
        "                           num_questions: int = 5,\n",
        "                           f1_threshold: float = 0.5) -> Dict:\n",
        "        \"\"\"\n",
        "        Main evaluation method for factual consistency.\n",
        "\n",
        "        Args:\n",
        "            clinical_dialogue: Original clinical dialogue (source)\n",
        "            generated_note: Generated SOAP note to evaluate\n",
        "            num_questions: Number of questions to generate\n",
        "            f1_threshold: F1 threshold for considering answers as matching\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with evaluation results\n",
        "        \"\"\"\n",
        "        # Step 1: Generate questions from generated note\n",
        "        print(\"Generating factual questions...\")\n",
        "        questions = self.generate_questions(generated_note, num_questions)\n",
        "\n",
        "        results = {\n",
        "            'questions': questions,\n",
        "            'answers_from_dialogue': [],\n",
        "            'answers_from_note': [],\n",
        "            'f1_scores': [],\n",
        "            'is_consistent': [],\n",
        "            'confidence_scores_dialogue': [],\n",
        "            'confidence_scores_note': []\n",
        "        }\n",
        "\n",
        "        # Step 2: For each question, get answers from both sources\n",
        "        print(f\"Answering {len(questions)} questions...\")\n",
        "        for i, question in enumerate(questions):\n",
        "            # Get answer from original dialogue (ground truth source)\n",
        "            answer_dialogue, conf_dialogue = self.extract_answer(\n",
        "                clinical_dialogue, question, 'qa'\n",
        "            )\n",
        "\n",
        "            # Get answer from generated note\n",
        "            answer_note, conf_note = self.extract_answer(\n",
        "                generated_note, question, 'qa'\n",
        "            )\n",
        "\n",
        "            # Calculate F1 overlap between answers\n",
        "            f1_score = self.calculate_f1(answer_note, answer_dialogue)\n",
        "\n",
        "            # Determine if answers are consistent (above threshold)\n",
        "            is_consistent = f1_score >= f1_threshold\n",
        "\n",
        "            # Store results\n",
        "            results['answers_from_dialogue'].append(answer_dialogue)\n",
        "            results['answers_from_note'].append(answer_note)\n",
        "            results['f1_scores'].append(f1_score)\n",
        "            results['is_consistent'].append(is_consistent)\n",
        "            results['confidence_scores_dialogue'].append(conf_dialogue)\n",
        "            results['confidence_scores_note'].append(conf_note)\n",
        "\n",
        "            print(f\"  Q{i+1}: {question[:50]}...\")\n",
        "            print(f\"    Dialogue Answer: {answer_dialogue[:50]}... (conf: {conf_dialogue:.3f})\")\n",
        "            print(f\"    Note Answer: {answer_note[:50]}... (conf: {conf_note:.3f})\")\n",
        "            print(f\"    F1: {f1_score:.3f}, Consistent: {is_consistent}\")\n",
        "\n",
        "        # Step 3: Calculate overall consistency score\n",
        "        consistency_score = sum(results['is_consistent']) / len(questions)\n",
        "\n",
        "        # Compile final results\n",
        "        final_results = {\n",
        "            'consistency_score': consistency_score,\n",
        "            'num_questions': len(questions),\n",
        "            'num_consistent': sum(results['is_consistent']),\n",
        "            'avg_f1_score': np.mean(results['f1_scores']) if results['f1_scores'] else 0,\n",
        "            'avg_confidence_dialogue': np.mean(results['confidence_scores_dialogue']) if results['confidence_scores_dialogue'] else 0,\n",
        "            'avg_confidence_note': np.mean(results['confidence_scores_note']) if results['confidence_scores_note'] else 0,\n",
        "            'detailed_results': results\n",
        "        }\n",
        "\n",
        "        print(f\"\\nOverall Consistency Score: {consistency_score:.3f} \"\n",
        "              f\"({sum(results['is_consistent'])}/{len(questions)} questions)\")\n",
        "\n",
        "        return final_results\n",
        "\n",
        "    def batch_evaluate(self,\n",
        "                      dialogues: List[str],\n",
        "                      generated_notes: List[str],\n",
        "                      num_questions: int = 5) -> Dict:\n",
        "        \"\"\"\n",
        "        Batch evaluation for multiple dialogue-note pairs.\n",
        "\n",
        "        Args:\n",
        "            dialogues: List of clinical dialogues\n",
        "            generated_notes: List of corresponding generated notes\n",
        "            num_questions: Number of questions per note\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with aggregated batch results\n",
        "        \"\"\"\n",
        "        if len(dialogues) != len(generated_notes):\n",
        "            raise ValueError(\"Number of dialogues must match number of generated notes\")\n",
        "\n",
        "        batch_results = []\n",
        "        all_scores = []\n",
        "\n",
        "        print(f\"Starting batch evaluation of {len(dialogues)} pairs...\")\n",
        "        for i, (dialogue, note) in enumerate(zip(dialogues, generated_notes)):\n",
        "            print(f\"\\nEvaluating pair {i+1}/{len(dialogues)}...\")\n",
        "\n",
        "            try:\n",
        "                result = self.evaluate_consistency(dialogue, note, num_questions)\n",
        "                batch_results.append(result)\n",
        "                all_scores.append(result['consistency_score'])\n",
        "            except Exception as e:\n",
        "                print(f\"Error evaluating pair {i+1}: {e}\")\n",
        "                # Add default result for failed evaluation\n",
        "                batch_results.append({\n",
        "                    'consistency_score': 0.0,\n",
        "                    'num_questions': num_questions,\n",
        "                    'num_consistent': 0,\n",
        "                    'avg_f1_score': 0.0,\n",
        "                    'error': str(e)\n",
        "                })\n",
        "                all_scores.append(0.0)\n",
        "\n",
        "        # Aggregate results\n",
        "        aggregated = {\n",
        "            'mean_consistency': np.mean(all_scores),\n",
        "            'std_consistency': np.std(all_scores),\n",
        "            'min_consistency': np.min(all_scores),\n",
        "            'max_consistency': np.max(all_scores),\n",
        "            'median_consistency': np.median(all_scores),\n",
        "            'num_samples': len(batch_results),\n",
        "            'individual_results': batch_results\n",
        "        }\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"BATCH EVALUATION SUMMARY\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"Mean Consistency: {aggregated['mean_consistency']:.3f} ± {aggregated['std_consistency']:.3f}\")\n",
        "        print(f\"Range: [{aggregated['min_consistency']:.3f}, {aggregated['max_consistency']:.3f}]\")\n",
        "        print(f\"Median: {aggregated['median_consistency']:.3f}\")\n",
        "        print(f\"Samples: {aggregated['num_samples']}\")\n",
        "\n",
        "        return aggregated\n",
        "\n",
        "\n",
        "evaluator = FactualConsistencyEvaluator(\n",
        "    qg_model_name=\"./models/fine-tuned-bart-qg\",\n",
        "    qa_model_name=\"./models/fine-tuned-roberta-qa\"\n",
        ")\n",
        "\n",
        "# Read data from files\n",
        "import json\n",
        "\n",
        "# Read dialogues (assuming JSON file with 'dialogues' key)\n",
        "with open('clinical_dialogues.json', 'r') as f:\n",
        "    dialogue_data = json.load(f)\n",
        "    clinical_dialogues = dialogue_data['dialogues']\n",
        "\n",
        "# Read generated notes (assuming JSON file with 'notes' key)\n",
        "with open('generated_notes.json', 'r') as f:\n",
        "    notes_data = json.load(f)\n",
        "    generated_notes = notes_data['notes']\n",
        "\n",
        "# Or read from CSV/text files\n",
        "# import pandas as pd\n",
        "# df = pd.read_csv('data.csv')\n",
        "# clinical_dialogues = df['dialogue'].tolist()\n",
        "# generated_notes = df['generated_note'].tolist()\n",
        "\n",
        "# Validate data length\n",
        "if len(clinical_dialogues) != len(generated_notes):\n",
        "    print(f\"Warning: Mismatched data - {len(clinical_dialogues)} dialogues vs {len(generated_notes)} notes\")\n",
        "    # Truncate to minimum length\n",
        "    min_len = min(len(clinical_dialogues), len(generated_notes))\n",
        "    clinical_dialogues = clinical_dialogues[:min_len]\n",
        "    generated_notes = generated_notes[:min_len]\n",
        "\n",
        "# Batch evaluation\n",
        "print(f\"Running evaluation on {len(clinical_dialogues)} samples...\")\n",
        "batch_results = evaluator.batch_evaluate(\n",
        "    dialogues=clinical_dialogues,\n",
        "    generated_notes=generated_notes,\n",
        "    num_questions=3  # Adjust as needed\n",
        ")\n",
        "\n",
        "# Save results to file\n",
        "output_file = 'factual_consistency_results.json'\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(batch_results, f, indent=2)\n",
        "\n",
        "print(f\"Evaluation complete. Results saved to {output_file}\")\n",
        "print(f\"Mean factual consistency: {batch_results['mean_consistency']:.3f} ± {batch_results['std_consistency']:.3f}\")"
      ]
    }
  ]
}