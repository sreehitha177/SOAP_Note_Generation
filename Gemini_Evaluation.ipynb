{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# GEMINI JUDGING\n",
        "# =============================================================================\n",
        "\n",
        "FLUENCY_PROMPT = \"\"\"\n",
        "You are an experienced physician and medical editor.\n",
        "\n",
        "Evaluate the FLUENCY of the following SOAP note.\n",
        "\n",
        "Consider:\n",
        "- grammatical correctness\n",
        "- clarity and coherence\n",
        "- professional medical tone\n",
        "- readability for clinical documentation\n",
        "\n",
        "Score on a scale from 0 to 10:\n",
        "- 0 = incoherent, ungrammatical, unusable\n",
        "- 10 = perfectly fluent, natural, indistinguishable from a human physician\n",
        "\n",
        "SOAP NOTE:\n",
        "<<<\n",
        "{generated_soap}\n",
        ">>>\n",
        "\n",
        "Return ONLY a single integer score between 0 and 10.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "CONSISTENCY_PROMPT = \"\"\"\n",
        "You are an experienced physician.\n",
        "\n",
        "Evaluate the CONSISTENCY of the generated SOAP note\n",
        "with respect to the reference physician SOAP note.\n",
        "\n",
        "Consider:\n",
        "- whether key clinical facts match\n",
        "- whether important information is missing\n",
        "- whether incorrect or fabricated details are introduced\n",
        "- whether assessment and plan align with the reference\n",
        "\n",
        "Score on a scale from 0 to 10:\n",
        "- 0 = completely inconsistent or incorrect\n",
        "- 10 = fully consistent and clinically aligned\n",
        "\n",
        "REFERENCE SOAP:\n",
        "<<<\n",
        "{reference_soap}\n",
        ">>>\n",
        "\n",
        "GENERATED SOAP:\n",
        "<<<\n",
        "{generated_soap}\n",
        ">>>\n",
        "\n",
        "Return ONLY a single integer score between 0 and 10.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7XifTfKGwDyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-generativeai\n",
        "\n",
        "import os\n",
        "import re\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyDmKkVhVFIuBcaxPdBpiZgOBqgmmVPbE_o\"\n",
        "\n",
        "\n",
        "\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "\n",
        "GEMINI_MODEL = \"gemini-2.0-flash-lite\"\n",
        "judge_model = genai.GenerativeModel(GEMINI_MODEL)\n",
        "\n",
        "def parse_score(text: str) -> int:\n",
        "    match = re.search(r\"\\b([0-9]|10)\\b\", text)\n",
        "    if match:\n",
        "        return int(match.group(1))\n",
        "    return 0  # fallback\n"
      ],
      "metadata": {
        "id": "8uBj_70DyHLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def judge_fluency(generated_soap: str, model: str = GEMINI_MODEL) -> int:\n",
        "    prompt = FLUENCY_PROMPT.format(generated_soap=generated_soap)\n",
        "\n",
        "    response = judge_model.generate_content(\n",
        "        prompt,\n",
        "        generation_config={\n",
        "            \"temperature\": 0.0,\n",
        "            \"max_output_tokens\": 5,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    output = response.text.strip()\n",
        "    return parse_score(output)\n",
        "\n"
      ],
      "metadata": {
        "id": "_mlN5AZlyBI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def judge_consistency(\n",
        "    reference_soap: str,\n",
        "    generated_soap: str,\n",
        "    model: str = GEMINI_MODEL,\n",
        ") -> int:\n",
        "    prompt = CONSISTENCY_PROMPT.format(\n",
        "        reference_soap=reference_soap,\n",
        "        generated_soap=generated_soap,\n",
        "    )\n",
        "\n",
        "    response = judge_model.generate_content(\n",
        "        prompt,\n",
        "        generation_config={\n",
        "            \"temperature\": 0.0,\n",
        "            \"max_output_tokens\": 5,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    output = response.text.strip()\n",
        "    return parse_score(output)"
      ],
      "metadata": {
        "id": "uKWgVTWEyBOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fluency_scores = []\n",
        "consistency_scores = []\n",
        "MAX_JUDGE_SAMPLES = 20\n",
        "\n",
        "for i, (ref, pred) in enumerate(\n",
        "    list(zip(refs, preds))[:MAX_JUDGE_SAMPLES]\n",
        "):\n",
        "    try:\n",
        "        fluency = judge_fluency(pred)\n",
        "        consistency = judge_consistency(ref, pred)\n",
        "\n",
        "        fluency_scores.append(fluency)\n",
        "        consistency_scores.append(consistency)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[Judge failed at sample {i}]: {e}\")\n",
        "        break\n",
        "\n",
        "\n",
        "results = {\n",
        "    \"fluency_mean\": round(float(np.mean(fluency_scores)), 3),\n",
        "    \"fluency_std\": round(float(np.std(fluency_scores)), 3),\n",
        "    \"consistency_mean\": round(float(np.mean(consistency_scores)), 3),\n",
        "    \"consistency_std\": round(float(np.std(consistency_scores)), 3),\n",
        "}\n",
        "\n",
        "print(results)"
      ],
      "metadata": {
        "id": "LY1OsjOtyBTq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}